model:
  # there might be differet to load from hf and resume from pl 
  # pretrained_checkpoint: "THUDM/CogVideoX-2b"
  base_learning_rate: 6e-6
  target: src.cogvideo_hf.cogvideo_pl.CogVideoXWorkFlow
  params: 
    # first stage model; cond stage model ; denoising model ; scheduler 
    first_stage_config:
      target: diffusers.AutoencoderKLCogVideoX
      params:
        pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b 
        subfolder: "vae"
    cond_stage_config:
      target: src.lvdm.modules.encoders.condition.FrozenT5Embedder
      params:
        version: "DeepFloyd/t5-v1_1-xxl"
        device: "cuda"
        max_length: 226 
        freeze: True
    # denosier config equal to unet config in vc 
    denoiser_config:
      target: diffusers.CogVideoXTransformer3DModel
      params:
        pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b 
        subfolder: "transformer"
        load_dtype: fp16 # bf16 5b fp16 2B 
        # revision: null 
        # variant: null
    adapter_config: # the whole dict is remoable 
        target: peft.LoraConfig
        params:
            r: 4
            lora_alpha: 1.0 
            init_lora_weights: True
            target_modules: ["to_k", "to_q", "to_v", "to_out.0"]

    # sampler config. Wrap it.       
    scheduler_config:
      target: diffusers.CogVideoXDPMScheduler
      params:
        pretrained_model_name_or_path: checkpoints/cogvideo/CogVideoX-5b
        subfolder: scheduler

## training config 
### data , can a toy dataset given 
data:
  target: src.data.lightning_data.DataModuleFromConfig
  params:
    batch_size: 2
    num_workers: 16
    wrap: false
    train:
      target: src.data.cogvideo_dataset.VideoDataset
      params:
        instance_data_root: "inputs/t2v/cogvideo/elon_musk_video"
        dataset_name: null 
        dataset_config_name: null
        caption_column: "labels.txt"
        video_column: "videos.txt"
        height: 480
        width: 720
        fps: 28
        max_num_frames: 2
        skip_frames_start: 0
        skip_frames_end: 0
        cache_dir: ~/.cache
        id_token: null

### training_step in cogvideoxft
lightning:
  trainer:
    benchmark: True
    num_nodes: 1
    accumulate_grad_batches: 2
    max_epochs: 2000
    precision: 32 # training precision
  callbacks:
    image_logger:
      target: src.utils.callbacks.ImageLogger
      params:
        batch_frequency: 100000
        max_images: 2
        to_local: True # save videos into files
        log_images_kwargs:
          unconditional_guidance_scale: 12 # need this, otherwise it is grey
    metrics_over_trainsteps_checkpoint:
      target: pytorch_lightning.callbacks.ModelCheckpoint
      params:
        filename: "{epoch:06}-{step:09}"
        save_weights_only: False
        # every_n_epochs: 300
        every_n_train_steps: 10
    

